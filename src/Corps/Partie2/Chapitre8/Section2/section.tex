

\subsection{La théorie des possibilités}


La théorie des \emph{sous-ensembles flous} ne permet pas, du moins
dans sa formulation initiale, de modéliser l'incertitude d'une
connaissance, seulement son \emph{imprécision.}

\textcite{Zadeh1978} a donc proposé en 1978 la \emph{théorie des
  possibilités,} qui permet d'étendre la théorie des sous-ensembles
flous en permettant la modélisation conjointe de \emph{l'imprécision}
et de \emph{l'incertitude.}

La \emph{théorie des possibilités} ne doit pas être confondue avec la
\emph{théorie des probabilités.} Si ces théories modélisent toutes
deux \emph{l'incertitude,} elles utilisent un formalisme et ont des
propriétés différentes. Cependant, ces deux théories restent
comparables, en plus de s’inscrire toutes deux dans le cadre plus
général \autocite{Bouchon-Meunier1995}, celui de la théorie des
fonctions de croyances \autocite{Shafer1976}.

\subsubsection{Mesures de possibilité et de nécessité}

Comme la \emph{théorie des probabilités} et la théorie des
\emph{fonctions de croyance,} la \emph{théorie des possibilités}
permet d'évaluer un doute sur la réalisation d'un (ou plusieurs)
\emph{événement(s)} donné(s). L'ensemble des événements étudiés sont
regroupés dans un ensemble fini \(X\), appelé \emph{ensemble des
  événements.} La \emph{théorie des possibilités} permet d'attribuer à
chaque événement défini dans \(X\) un coefficient, compris entre 0 et
1, évaluant la \emph{possibilité} de l'événement. Une valeur de 1
indiquant que l'événement est \enquote{tout à fait possible}
\autocite[p. 43]{Bouchon-Meunier2007} et une valeur nulle, qu'il est
impossible. Bien que proches, la notion de possibilité ne doit pas
être confondue celle de probabilité. La probabilité quantifie, en
effet, la \enquote{chance} de réalisation d'un événement, alors que la
possibilité ne quantifie que la plausibilité de ce même événement. Dit
autrement, une probabilité maximale indique que l'événement considéré
va se réaliser, alors qu'une possibilité maximale n'indique que qu'il
certain que cet événement puisse se produire et non qu'il se produira.
Le coefficient de possibilité est attribué par une fonction \(Π\),
nommée \emph{mesure de possibilité,} qui l'attribue à chaque élément
de l'ensemble des parties de \emph{l'ensemble des événements}
(\(Π : P(X) → [0,1]\)). La fonction \(Π\) doit nécessairement :

\begin{itemize}
\item Attribuer un coefficient nul à \emph{l'ensemble vide} :
  \(Π(∅)=0\)
\item Attribuer un coefficient de 1 à \emph{l'ensemble des événements
    :} \(Π(X)=1\)
\item Définir la \emph{possibilité} conjonctive de deux événements
  (\(A\) et \(B\)) comme la \emph{possibilité} de l'événement le plus
  possible : \(∀ A,B ∈ P(X),\ Π(A ∪ B) = \max(Π(A),\ Π(B))\)
\end{itemize}

On peut déduire de ces trois propriétés \autocite{Bouchon-Meunier2007}
que la possibilité disjointe de deux événements \(A\) et \(B\) est
toujours inférieure à la \emph{possibilité} de l'événement le moins
possible :
%
\begin{equation}
  ∀ A,\ B ∈ P(X),\ Π(A ∩ B) ≤ \min(Π(A),\ Π(B))
\end{equation}
%
Il est par conséquent possible que deux événements aient
respectivement une possibilité non nulle, mais que leur occurrence
simultanée soit nulle (\( Π(A ∩ B) = 0\)) n'implique pas que \(Π(A)\)
et \(Π(B)\) soient égaux à 0). Une autre propriété déductible est que
les \emph{mesures de possibilités} sont \emph{monotones} relativement
à l'inclusion des parties de \(X\). Ce qui implique que si un
événement \(A\) est inclus dans un événement \(B\) (\eg si \(A\) est
la \emph{possibilité} qu'il pleuve en fin de journée, \(B\) pourrait
être la \emph{possibilité} que le temps change en fin de journée),
alors la possibilité de \(B\) est supérieure à celle de \(A\) :
%
\begin{equation}
  A \subseteq B, Π(A) ≤ Π(B)
\end{equation}
%
Ce qui revient à dire que la probabilité que le temps change (\(B\))
est supérieure à la \emph{possibilité} qu'il pleuve (\(A\)).

Une autre propriété des \emph{mesures de possibilités} est la faible
influence de la \emph{possibilité} d'un événement \(A\) sur la
\emph{possibilité} de son complémentaire \(A^C\). En effet, quelque
soit l’événement traité, lui ou son contraire est \enquote{tout à fait
  possible}, par conséquent :
%
\begin{equation}
  \label{eq:poss_cont}
  ∀ A ∈ P(X),\ \max(Π(A),\ Π(A^C)) = 1  
\end{equation}
%
Il est donc possible que la somme des \emph{possibilités} de \(A\) et
de \(A^C\) soit supérieure à 1. Cette propriété diffère fortement de
ce que l'on peut trouver en \emph{théories des probabilités} où la
probabilité du contraire d'un événement est plus contrainte, puisque
la somme de ces deux probabilités et toujours égale à 1. En
\emph{théorie des possibilités} il est simplement nécessaire qu'une
des \emph{possibilité} soit de 1. Il est par conséquent possible qu'un
événement et son contraire soient tous les deux parfaitement
possibles, ce qui traduit une situation d'ignorance absolue quant à la
réalisation de l'événement considéré.

\textcite{Bouchon-Meunier1995} illustre les \emph{mesures de
  possibilités} avec l'exemple de la réception d'un colis. On peut
définir une \emph{mesure de possibilité} donnant la susceptibilité que
le colis soit reçu un jour donné de la semaine. Dans ce cas,
\emph{l'ensemble des événements} \(X\) correspond à l'ensemble des
jours de la semaine et les parties de \(X\) aux combinaisons de ces
jours (\eg \(\{\text{lundi},\ \text{mardi}\}\),
\(\{\text{jeudi, vendredi, samedi}\}\), \emph{etc.})  On peut
attribuer une \emph{mesure de possibilité} à ces différentes parties
de \(X\). Si l'on reprend les valeurs proposées par
\textcite{Bouchon-Meunier1995} alors :
% 
\begin{itemize}
\item \(\Pi(\{\text{lundi, mardi}\})=1\)
\item \(\Pi(\{\text{mercredi}\})=0,8\)
\item \(\Pi(\{\text{jeudi}\})=0,2\)
\item \(\Pi(\{\text{vendredi, samedi, dimanche}\})=0\)
\end{itemize}
%
Ces valeurs signifient qu'il est \enquote{tout à fait possible} que le
colis arrive en début de semaine, relativement possible qu'il arrive
le mercredi, peu possible qu'il arrive le jeudi et impossible qu'il
arrive en fin de semaine. Comme on peut le remarquer il n'est pas
nécessaire d'attribuer une mesure de \emph{possibilité} à toutes les
parties de \(X\), cela n'empêche pas d'estimer les \emph{possibilités}
d'autres parties de \(X\), par exemple la \emph{possibilité} que le
colis arrive un mercredi ou un jeudi est de 0,8 \footnote{Soit le
  maximum des deux valeurs, conformément à la règle définissant la
  possibilité conjonctive.} ou la possibilité qu'il arrive le dimanche
est nulle \footnote{Étant donné que
  \(\Pi(\{\text{vendredi, samedi, dimanche}\})=0\) et que l'union
  possibilités est réalisée avec le maximum, cela implique que ni le
  vendredi, ni le samedi, ni le dimanche ont une possibilité non
  nulle.}. Il n'est cependant pas possible de connaitre précisément
certaines possibilités, comme par exemple celle que le colis arrive un
mardi. Il serait toutefois possible de calculer la possibilité de
chaque partie de \(X\) si l'on connaissait la possibilité de chaque
événement singleton (\ie la possibilité que le colis arrive lundi,
mardi, mercredi, \emph{etc.}), la possibilité des événements
composites (\eg la possibilité que le colis arrive lundi ou dimanche)
étant calculable en combinant les événements singletons.

Dans cette situation, c'est-à-dire lorsqu'une \emph{mesure de
  possibilité} attribue à chaque \emph{événement} singleton
\footnote{C'est-à-dire lorsque la possibilité n'est attribuée qu'aux
  éléments de \(X\) et non aux éléments des parties de \(X\).}) un
coefficient de possibilité, on parle de mesure de possibilité
\enquote{totalement définie} \autocite{Bouchon-Meunier2007}. Pour
attribuer un \emph{coefficient de possibilité} à chaque événement de
\(X\) on définit une \emph{distribution de possibilité}
(\(π : X → [0,1]\)), dont le supremum pour un événement \(x\) de \(X\)
doit être égal à 1 :
%
\begin{equation}
  \sup_{x ∈ X}π(x)=1
\end{equation}

À l'exception de ce dernier point, toutes ces propriétés sont
partagées par les \emph{fonctions d'appartenance}
(\autoref{sec:3-2}). Ainsi, dans le cas où une \emph{fonction
  d'appartenance} respecte cette dernière condition, c'est-à-dire
qu'elle soit normalisée \footnote{Contrairement à une
  \emph{distribution de possibilité,} une fonction d'appartenance
  n'est pas tenue d'avoir 1 pour supremum. On parle de
  \emph{sous-ensemble flou} normalisé quant au moins un de ces élément
  à un degré d’appartenance égal à un
  \autocite{Bouchon-Meunier2007}.}, elle est équivalente à une
\emph{distribution de possibilités.}

On peut reconstruire la \emph{mesure de possibilité} de chaque
événement à partir de la \emph{distribution de possibilités,} la
\emph{mesure de possibilité} d'un événement \(A\) est alors égale à la
valeur la plus importante de la \emph{distribution des possibilités}
sur cet intervalle, soit :

\begin{equation}
  \Pi(A) = \sup_{x \in A}\pi(x)
\end{equation}

Par exemple, on peut définir une \emph{distribution de possibilité}
donnant la \emph{possibilité} qu'un colis soit livré un jour donné
(\autoref{fig:fnc_possib}). À partir d'une telle fonction on peut
calculer la \emph{mesure de possibilité} de chaque partie de
\emph{l'ensemble des événements.} Par exemple, la possibilité que le
\emph{colis} soit livré entre jeudi et dimanche et de 0,2, puisque
c'est la valeur maximale prise par la fonction \(\pi\) sur cette
période et la possibilité que le colis ne soit pas livré un jeudi est
égale à la valeur maximale de la distribution de possibilité pour tous
les jours à l'exception du jeudi, soit 1.

\begin{figure}
  \centering
  \input{../figures/fnc_possib.tex}
  \caption{Exemple d'une distribution de possibilité par paliers : la
    possibilité qu'un colis arrive chaque jours de la semaine.}
  \label{fig:fnc_possib}
\end{figure}

La mesure de possibilité ne permet pas à elle seule de quantifier la
\emph{certitude} d'un événement, elle ne renseigne que sur sa capacité
de réalisation. Si l'on reprend l’exemple du colis, mais en ne
conservant que la possibilité que le colis arrive le lundi. Le fait
que cette possibilité soit maximale ne donne pas suffisamment
d'informations pour savoir si le colis va réellement arriver le lundi,
tout dépend de la valeur prise par l'événement contraire (\ie le colis
n'arrive pas lundi). Si nous considérons que la possibilité que le
colis n'arrive pas lundi est nulle, alors cela traduit le fait que le
colis ne peut pas arriver un autre jour de la semaine, il peut donc
arriver le lundi (car : \(\Pi(\{\text{lundi}\}=1\)) et ne peut pas
arriver un autre jour (car : \(\Pi\{\text{mardi, …, dimanche}\}=0\)),
on est donc \emph{certains} que le colis arrivera lundi. À l'inverse,
si la possibilité que le colis n'arrive pas lundi est également de 1
(\ie \(\Pi\{\text{mardi, …, dimanche}\}=1\)) alors le colis peut aussi
bien arriver le lundi que n'importe quel autre jour de la semaine, on
n'a donc aucune idée de ce qui va réellement se produire, notre
incertitude sur cet événement est totale.

La certitude d'un événement ne peut donc être jugée qu'à l'aune de
deux mesures, la possibilité d'un événement et la possibilité de
l'événement contraire. On définit donc une seconde mesure, \emph{la
  nécessité,} quantifiant la certitude que l'on a sur la réalisation
d'un événement donné. La \emph{nécessité} est une mesure duale de la
possibilité et elle est exprimée à partir de la possibilité du
contraire de l'événement étudié :

\begin{equation}
  \label{eq:nec_comp}
  \forall A \in P(X), N(A) = 1 - \Pi(A^C)
\end{equation}

Comme la \emph{mesure de possibilité,} une \emph{mesure de nécessité}
est une fonction (\(N\)) attribuant à chaque partie de l'ensemble des
événements un degré, compris entre 0 et 1
(\(N : P(X) \rightarrow [0,1]\)). Cette fonction \(N\) doit également
:

\begin{itemize}
\item Attribuer un coefficient nul à \emph{l'ensemble vide} :
  \(N(∅)=0\)
\item Attribuer un coefficient de 1 à \emph{l'ensemble des événements
    :} \(N(X)=1\)
\item Définir la \emph{nécessité} disjonctive de deux événements
  (\(A\) et \(B\)) comme la \emph{nécessité} de l'événement le moins
  nécessaire : \(∀ A,B ∈ P(X),\ N(A \cap B) = \min(N(A),\ N(B))\)
\end{itemize}

Ce qui implique que plus l'événement complémentaire de \(A\) est
\emph{possible,} moins \(A\) est nécessaire. Pour reprendre l'exemple
précédent, si la \emph{possibilité} que le colis arrive lundi est de 1
et que la \emph{possibilité} qu'il n'arrive pas lundi est également de
1, alors la \emph{nécessité} de cet événement est nulle. Dans cette
configuration, il est parfaitement possible que le colis arrive lundi
(la possibilité est maximale) mais on n'en est absolument pas sûrs la
nécessité est nulle). Si la \emph{possibilité} que le colis n'arrive
pas lundi est nulle, alors la \emph{nécessité} qu'il arrive lundi est
de 1, c'est-à-dire qu'on a une confiance absolue dans la réalisation
de cet événement. La dualité de ces deux mesures contraint cependant
leurs valeurs respectives. Tout d'abord la \emph{nécessité} ne peut
pas être plus élevée que la \emph{possibilité} \footnote{Ce qui
  traduirait une situation, absurde, où un événement doit arriver mais
  ne le peut.}, mais les deux valeurs peuvent cependant être égales,
comme dans le cas d'une certitude absolue en la réalisation d'un
événement \footnote{Dans ce cas la certitude et la nécessité ont une
  valeur de 1.}, par conséquent :

\begin{equation}
  \forall A \in P(X), N(A) ≤ \Pi(A)
\end{equation}

De plus, comme la nécessité d'un événement est liée à la possibilité
de l'événement opposé (\autoref{eq:nec_comp}) et que la possiblité
d'un événement et de son opposé sont liées (\autoref{eq:poss_cont})
alors la nécessité et la possibilité sont également liées :

\begin{equation}
  \forall A \in P(X), \max(\Pi(A), 1-N(A)) = 1
\end{equation}

Cette caractéristique à deux conséquences majeures :

\begin{itemize}
\item S'il existe une incertitude alors la possibilité est maximale ;
  \(\text{si : } N(A) ≠ 0, \text{ alors : } Π(A)=1\)
\item Si l'événement n'est pas \enquote{tout à fait possible} alors il
  n'est pas nécessaire ;
  \(\text{si : } Π(A) ≠ 1, \text{ alors : } N(A)=0\)
\end{itemize}

Ces deux propriétés impliquent que dès qu'un événement est, ne
serai-ce qu'un peu, certain (\(N(A) ≠ 0\) alors il est parfaitement
possible (\(Π(A)=1\)). De même si un événement n'est pas parfaitement
possible (\(Π(A) ≠ 1\) alors on ne peut pas être certain
(\(N(A) = 0\)) qu'il se produira.

Étant donné que la nécessité sont des mesures duales, on peut définir
la mesure de nécessité (\(N(A)\)) à partir de la distribution de
possibilité (\(π\)) :

\begin{equation}
  N(A) = \inf_{x ∉ A}(1-π(x)) = 1 - \sup_{x ∉ A}π(x)
\end{equation}

Ainsi, une seule distribution (\(\pi\)) est nécessaire pour calculer
les mesures de possibilité et de nécessité de chaque événement. Par
exemple, si l'on reprend la distribution de possibilité proposée par
la \autoref{fig:fnc_possib}, on peut calculer la nécessité de chaque
événement. Par exemple, si la possibilité que le colis arrive le lundi
est de 1, la nécessité de ce même événement est de 0,2, puisque que le
minimum de la fonction \(1-π(x)\), pour tous les événements à
l'exception de celui étudié est de 0,2 (ce qui correspond à :
\(1-π(\{\text{mardi}\})\)).

\subsubsection{Modélisation conjointe de \emph{l'incertitude} et de
  \emph{l'imprécision}}

On peut combiner le formalisme de la théorie des possibilités à la
théorie des sous-ensembles flous pour permettre la modélisation
conjointe de \emph{l'imprécision} et de \emph{l'incertitude.}



Comme nous l'avons déjà indiqué, une fonction d'appartenance
normalisée est une distribution de possibilité,

Dans notre cas, on cherche à utiliser la théorie des possibilités pour
représenter le doute du secouriste sur la véracité de l'indice de
localisation donné par le requérant. Ce parti-pris a donc une
implication majeure. Comme les sous-ensembles flous que nous
manipulons représentent des \emph{zones de localisation,} alors la
certide

Dans le cas où \emph{l'incertitude} (\(i\)) est homogène, la
\emph{distribution de possibilité} (\(π\)) de cette proposition est
obtenue en tronquant la base de la fonction d'appartenance par la
droite d'ordonnée \(i\) \autocite{Bouchon-Meunier2007}. La
\emph{distribution de possibilité} (\(π'\)) combinant incerti XX est
donc :
%
\begin{equation}
  π'(x) = \max(π(x),\ i).  
\end{equation}


La \autoref{fig:combinaison_incertitude_imprecision} illustre cette
combinaison. On définit une distribution de possibilité homogène sur
l'espace de la métrique et une fonction d'appartenance, permettant de
définir un \emph{sélecteur,} utilisé lors de la spatialisation. Si
l'on combine ces deux distributions on obtient une fonction
d'appartenance \enquote{rehaussée} par l'incertitude. Dans cette
configuration, le degré d'appartenance ne pourra être supérieur à la
valeur de 

\begin{figure}
  \centering
  \input{../figures/fnc_incert_floue.tex}
  \caption{SS}
  \label{fig:combinaison_incertitude_imprecision}
\end{figure}


La confiance peut se définir en amont de la spatialisation.

Diminuer la confiance c'est augmenter le degré d'appartenance minimal

Proposer une version améliorée de la fig finale du chapitre 4 avec la
confiance.

Dire que les fonctions présentées reviennent a considérer que la
confiance est absolue

Parler des seuils de la confiance (3) et des valeurs associées ou
contraintes.


\subsection{La modélisation de la confiance d'un \emph{indice de
    localisation}}

Pour permettre la prise en compte de la confiance


La confiance donnée par le secouriste portant sur \emph{l'indice de
  localisation} et non sur l'une de ces composantes (\eg \emph{objet
  de référence,} \emph{relation de localisation,} \emph{etc.}), le
degré de confiance est distribué de manière homogène, chaque position
de la \ac{zir} se voyant attribuer la même valeur.


La modification des degrés d'appartenance des positions est
équivalente à une modification de la forme de la fonction
d'appartenance utilisée pour \emph{spatialiser l'indice de
  localisation} (\autoref{fig:fnc_incert}). Comme pour les
\emph{modifieurs} (\autoref{chap:07}), la variation du seuil de
confiance modifie la forme générale de la courbe, mais ne change pas
les paramètres du \emph{fuzzyficateur,} que sont l'écartement
(\(\delta\)) et la valeur de référence (\(v\)). Toutefois,
contrairement aux \emph{modifieurs,} la modification de la confiance à
également pour effet de changer la valeur de seuils de la fonction
d'appartenance. En effet, si la pente des fonctions appartenance est
conservée lors du rehaussement de la valeur minimale, ce n'est pas le
cas de la valeur où la fonction d'appartenance atteint la valeur
nulle, cette dernière étant rehaussée par la diminution de la
confiance.

\begin{figure}
  \centering  \subfloat[\label{fig:fnc_app_inc}]{\input{../figures/fnc_app_inc.tex}}\hfill
  \subfloat[\label{fig:fnc_app_inc_2}]{\input{../figures/fnc_app_inc_2.tex}}

  \subfloat[\label{fig:fnc_app_inc_3}]{\input{../figures/fnc_app_inc_3.tex}}\hfill
  \subfloat[\label{fig:fnc_app_inc_4}]{\input{../figures/fnc_app_inc_4.tex}}
  %
  \caption[Représentation des \emph{fonctions d'appartenance} de
  différents \emph{fuzzyfieurs} avec une
  \emph{incertitude}]{Représentation des \emph{fonctions
      d'appartenance} des \emph{fuzzyfieurs}
    \protect\onto[orla]{Eq\-Val} \protect\subref{fig:fnc_app_inc},
    \protect\onto[orla]{Sup\-Val} \protect\subref{fig:fnc_app_inc_2},
    \protect\onto[orla]{Inf\-Val} \protect\subref{fig:fnc_app_inc_3}
    et \protect\onto[orla]{Eq\-Val} avec le \emph{modifieur}
    \protect\onto[orla]{Not} \protect\subref{fig:fnc_app_inc_4}, avec
    un degré de \emph{certitude} (\(i\)) variable (respectivement
    0,8, 0,4, 0,2 et 0,5).}
  \label{fig:fnc_incert}
\end{figure}

On peut illustrer les effets de cette modélisation en comparant le
résultat de la \emph{spatialisation} d'un même \emph{indice de
  localisation} avec une confiance variable. La
\autoref{fig:zlc_cert_vs_inceret} illustre la \emph{spatialisation} de
la \emph{relation de localisation atomique} \onto[orla]{Pres\-De}
(comme pour les figures \ref{fig:tiers-exclu} et
\ref{fig:non-contradiction}) à partir du même pixel, avec la même
fonction d'appartenance. Cependant, alors qu'une confiance absolue est
donnée au premier cas, c'est une confiance partielle (0,4) qui est
donnée à la seconde modélisation. Par conséquent, la valeur minimale
que peut prendre une position est de 0,6. Si cette approche permet de
XXXXX, c'est par le jeu des opérations inter-zones qu'elle prend tout
son sens.

\begin{figure}
  \centering
  \subfloat[]{\input{../figures/zlc_certaine.tex}}\hspace{2cm}
  \subfloat[]{\input{../figures/zlc_incertaine.tex}}
  \caption{Modélisation d'un même \emph{relation de localisation
      atomique} (\protect\onto[orla]{Pres\-De}), à partir du même
    \emph{objet de référence} avec deux certitudes différentes,
    absolue et partielle (0,4).}
  \label{fig:zlc_cert_vs_inceret}
\end{figure}

En rehaussant la valeur du degré d'appartenance minimal à la \ac{zlc},
la m (\autoref{fig:intersection}).

\begin{figure}
  \centering
  \subfloat[]{\input{../figures/comparaison_intersection_incertitude.tex}}

  \subfloat[]{\input{../figures/comparaison_intersection_incertitude_2.tex}}
  \caption{Influence de l'incertitude d'un \emph{indice de
      localisation} sur la \emph{zone de localisation} résultant de la
    fusion de deux \ac{zlc}.}
  \label{fig:intersection}
\end{figure}



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../../../main"
%%% End:
