

\subsection{La théorie des possibilités}


La théorie des \emph{sous-ensembles flous} ne permet pas, du moins
dans sa formulation initiale, de modéliser l'incertitude d'une
connaissance, seulement son \emph{imprécision.}

\textcite{Zadeh1978} a donc proposé en 1978 la \emph{théorie des
  possibilités,} qui permet d'étendre la théorie des sous-ensembles
flous en permettant la modélisation conjointe de \emph{l'imprécision}
et de \emph{l'incertitude.}

La \emph{théorie des possibilités} ne doit pas être confondue avec la
\emph{théorie des probabilités.} Si ces théories modélisent toutes
deux \emph{l'incertitude,} elles utilisent un formalisme et ont des
propriétés différentes. Cependant, ces deux théories restent
comparables, en plus de s’inscrire toutes deux dans le cadre plus
général \autocite{Bouchon-Meunier1995}, celui de la théorie des
fonctions de croyances \autocite{Shafer1976}.

\subsubsection{Mesures de possibilité et de nécessité}

Comme la \emph{théorie des probabilité} (et donc des \emph{fonctions
  de croyance}), la \emph{théorie des possibilités} permet évaluer la
\enquote{chance} qu'un événement donné se produise. Ces événements
sont regroupés dans un ensemble fini \(X\), appelé \emph{ensemble des
  événements.} La \emph{théorie des possibilités} permet d'attribuer à
chaque événement défini dans \(X\) un coefficient, compris entre 0 et
1, évaluant la possibilité de l'événement. Une valeur de 1 indiquant
que l'événement est \enquote{tout à fait possible}
\autocite[p. 43]{Bouchon-Meunier2007} et une valeur nulle, qu'il est
impossible. Ce coefficient est attribué par une fonction \(Π\), nommée
\emph{mesure de possibilité,} qui l'attribue à chaque élément de
l'ensemble des parties de \emph{l'ensemble des événements}
(\(Π : P(X) → [0,1]\)). La fonction \(Π\) doit nécessairement :

\begin{itemize}
\item Attribuer un coefficient nul à \emph{l'ensemble vide} :
  \(Π(∅)=0\)
\item Attribuer un coefficient de 1 à \emph{l'ensemble des événements
    :} \(Π(X)=1\)
\item Définir la \emph{possibilité} conjonctive de deux événements
  (\(A\) et \(B\)) comme la \emph{possibilité} de l'événement le plus
  possible : \(∀ A,B ∈ P(X),\ Π(A ∪ B) = \max(Π(A),\ Π(B))\)
\end{itemize}

On peut déduire de ces trois propriétés \autocite{Bouchon-Meunier2007}
que la possibilité disjointe de deux événements \(A\) et \(B\) est
toujours inférieure à la \emph{possibilité} de l'événement le moins
possible :
%
\begin{equation}
  ∀ A,\ B ∈ P(X),\ Π(A ∩ B) ≤ \min(Π(A),\ Π(B))
\end{equation}
%
Il est par conséquent possible que deux événements aient
respectivement une possibilité non nulle, mais que leur occurrence
simultanée soit nulle (\( Π(A ∩ B) = 0\)) n'implique pas que \(Π(A)\)
et \(Π(B)\) soient égaux à 0). Une autre propriété déductible est que
les \emph{mesures de possibilités} sont \emph{monotones} relativement
à l'inclusion des parties de \(X\). Ce qui implique que si un
événement \(A\) est inclus dans un événement \(B\) (\eg si \(A\) est
la \emph{possibilité} qu'il pleuve en fin de journée, \(B\) pourrait
être la \emph{possibilité} que le temps change en fin de journée),
alors la possibilité de \(B\) est supérieure à celle de \(A\) :
%
\begin{equation}
  A \subseteq B, Π(A) ≤ Π(B)
\end{equation}
%
Ce qui revient à dire que la probabilité que le temps change (\(B\))
est supérieure à la \emph{possibilité} qu'il pleuve (\(A\)).

Une autre propriété des \emph{mesures de possibilités} est la faible
influence de la \emph{possibilité} d'un événement \(A\) sur la
\emph{possibilité} de son complémentaire \(A^C\). En effet, quelque
soit l’événement traité, lui ou son contraire est \enquote{tout à fait
  possible}, par conséquent :
%
\begin{equation}
  \label{eq:poss_cont}
  ∀ A ∈ P(X),\ \max(Π(A),\ Π(A^C)) = 1  
\end{equation}
%
Il est donc possible que la somme des \emph{possibilités} de \(A\) et
de \(A^C\) soit supérieure à 1. Cette propriété diffère fortement de
ce que l'on peut trouver en \emph{théories des probabilités} où la
probabilité du contraire d'un événement est plus contrainte, puisque
la somme de ces deux probabilités et toujours égale à 1. En
\emph{théorie des possibilités} il est simplement nécessaire qu'une
des \emph{possibilité} soit de 1.

\textcite{Bouchon-Meunier1995} illustre les \emph{mesures de
  possibilités} avec l'exemple de la réception d'un colis. On peut
définir une \emph{mesure de possibilité} donnant la susceptibilité que
le colis soit reçu un jour donné de la semaine. Dans ce cas,
\emph{l'ensemble des événements} \(X\) correspond à l'ensemble des
jours de la semaine et les parties de \(X\) aux combinaisons de ces
jours (\eg \(\{\text{lundi},\ \text{mardi}\}\),
\(\{\text{jeudi, vendredi, samedi}\}\), \emph{etc.})  On peut
attribuer une \emph{mesure de possibilité} à ces différentes parties
de \(X\). Si l'on reprend les valeurs proposées par
\textcite{Bouchon-Meunier1995} alors :
% 
\begin{itemize}
\item \(\Pi(\{\text{lundi, mardi}\})=1\)
\item \(\Pi(\{\text{mercredi}\})=0,8\)
\item \(\Pi(\{\text{jeudi}\})=0,2\)
\item \(\Pi(\{\text{vendredi, samedi, dimanche}\})=0\)
\end{itemize}
%
Ces valeurs signifient qu'il est \enquote{tout à fait possible} que le
colis arrive en début de semaine, relativement possible qu'il arrive
le mercredi, peu possible qu'il arrive le jeudi et impossible qu'il
arrive en fin de semaine. Comme on peut le remarquer il n'est pas
nécessaire d'attribuer une mesure de \emph{possibilité} à toutes les
parties de \(X\), cela n'empêche pas d'estimer les \emph{possibilités}
d'autres parties de \(X\), par exemple la \emph{possibilité} que le
colis arrive un mercredi ou un jeudi est de 0,8 \footnote{Soit le
  maximum des deux valeurs, conformément à la règle définissant la
  possibilité conjonctive.} ou la possibilité qu'il arrive le dimanche
est nulle \footnote{Étant donné que
  \(\Pi(\{\text{vendredi, samedi, dimanche}\})=0\) et que l'union
  possibilités est réalisée avec le maximum, cela implique que ni le
  vendredi, ni le samedi, ni le dimanche ont une possibilité non
  nulle.}. Il n'est cependant pas possible de connaitre précisément
certaines possibilités, comme par exemple celle que le colis arrive un
mardi. Il est toutefois possible de calculer la possibilité de chaque
partie de \(X\) en ne connaissant que la possibilité individuelle de
chaque événement, ici la possibilité que le colis arrive chaque jour
de la semaine, pris indépendamment des autres.

Lorsqu'une \emph{mesure de possibilité} attribue à chaque
\emph{événement} de \(X\) un coefficient de possibilité, on parle de
mesure \enquote{totalement définie}
\autocite{Bouchon-Meunier2007}. Pour attribuer un \emph{coefficient de
  possibilité} à chaque événement de \(X\) on définit une
\emph{distribution de possibilité} (\(π : X → [0,1]\)), dont le
supremum pour un événement \(x\) de \(X\) doit être égal à 1 :
%
\begin{equation}
  \sup_{x ∈ X}π(x)=1
\end{equation}

À l'exception de ce dernier point, toutes ces propriétés sont
partagées par les \emph{fonctions d'appartenance}
(\autoref{sec:3-2}). Ainsi, dans le cas où une \emph{fonction
  d'appartenance} respecte cette dernière condition, c'est-à-dire
qu'elle soit normalisée \footnote{Contrairement à une
  \emph{distribution de possibilité,} une fonction d'appartenance
  n'est pas tenue d'avoir 1 pour supremum. On parle de
  \emph{sous-ensemble flou} normalisé quant au moins un de ces élément
  à un degré d’appartenance égal à un
  \autocite{Bouchon-Meunier2007}.}, elle est équivalente à une
\emph{distribution de possibilités.}

On peut reconstruire la \emph{mesure de possibilité} de chaque
événement à partir de la \emph{distribution de possibilités,} la
\emph{mesure de possibilité} d'un événement \(A\) est alors égale à la
valeur la plus importante de la \emph{distribution des possibilités}
sur cet intervalle, soit :

\begin{equation}
  \Pi(A) = \sup_{x \in A}\pi(x)
\end{equation}

Par exemple, on peut définir une \emph{distribution de possibilité}
par paliers donnant la \emph{possibilité} qu'un colis soit livré un
jour donné (\autoref{fig:fnc_possib}). À partir d'une telle fonction
on peut calculer la \emph{mesure de possibilité} de chaque partie de
\emph{l'ensemble des événements.} Par exemple, la possibilité que le
\emph{colis} ne soit pas livré un jeudi est égale à la valeur maximale
de la distribution de possibilité pour tous les jours à l'exception du
jeudi, soit 1.

\begin{figure}[hb]
  \centering
  \input{../figures/fnc_possib.tex}
  \caption{Exemple d'une distribution de possibilité par paliers : la
    possibilité qu'un colis arrive chaque jours de la semaine.}
  \label{fig:fnc_possib}
\end{figure}

La \emph{mesure de possibilité} est associée à une seconde mesure, la
\emph{nécessité.} Cette mesure permet de compléter l'information
donnée par la \emph{possibilité.} En effet, même si un événement \(A\)
à une \emph{possibilité} de 1 (\eg 1 pour le colis arrive lundi), la
possibilité de son complémentaire \(A^C\) (\ie le colis n'arrive pas
lundi) peut prendre n'importe quelle autre valeur, y compris 1, ce qui
indique que les deux événements sont tout à fait possibles,
c'est-à-dire que l'on ne sait rien, ou 0, ce qui indique une certitude
absolue dans la réception du colis, c'est-à-dire qu'il ne peut en être
autrement \autocite{Bouchon-Meunier1995}. La \emph{mesure de
  nécessité} a pour fonction de quantifier la \emph{certitude} d'un
événement, elle permet donc de distinguer une situation d'ignorance
totale, d'une situation de connaissance absolue. 

Comme la \emph{mesure de possibilité,} une \emph{mesure de nécessité}
est une fonction (\(N\)) attribuant à chaque partie de l'ensemble des
événements un degré, compris entre 0 et 1
(\(N : P(X) \rightarrow [0,1]\)). Cette fonction \(N\) doit également
:

\begin{itemize}
\item Attribuer un coefficient nul à \emph{l'ensemble vide} :
  \(N(∅)=0\)
\item Attribuer un coefficient de 1 à \emph{l'ensemble des événements
    :} \(N(X)=1\)
\item Définir la \emph{nécessité} disjonctive de deux événements
  (\(A\) et \(B\)) comme la \emph{nécessité} de l'événement le moins
  nécessaire : \(∀ A,B ∈ P(X),\ N(A \cap B) = \min(N(A),\ N(B))\)
\end{itemize}

La \emph{nécessité} et la \emph{possibilité} sont des mesures duales,
la \emph{nécessité} d'un événement \(A\) est donc fortement liée à la
\emph{possibilité} du complémentaire de cet événement, \(A^C\). La
nécessité de \(A\) peut donc s'exprimer de la manière suivante :

\begin{equation}
  \label{eq:nec_comp}
  \forall A \in P(X), N(A) = 1 - \Pi(A^C)
\end{equation}

Ce qui implique que plus l'événement complémentaire de \(A\) est
\emph{possible,} moins \(A\) est nécessaire. Pour reprendre l'exemple
précédent, si la \emph{possibilité} que le colis arrive lundi est de 1
et que la \emph{possibilité} qu'il n'arrive pas lundi est également de
1, alors la \emph{nécessité} de cet événement est nulle. Dans cette
configuration, il est parfaitement possible que le colis arrive lundi
(la possibilité est maximale) mais on n'en est absolument pas sûrs la
nécessité est nulle). Si la \emph{possibilité} que le colis n'arrive
pas lundi est nulle, alors la \emph{nécessité} qu'il arrive lundi est
de 1, c'est-à-dire qu'on a une confiance absolue dans la réalisation
de cet événement. La dualité de ces deux mesures contraint cependant
leurs valeurs respectives. Tout d'abord la \emph{nécessité} ne peut
pas être plus élevée que la \emph{possibilité} \footnote{Ce qui
  traduirait une situation, absurde, où un événement doit arriver mais
  ne le peut.}, mais les deux valeurs peuvent cependant être égales,
comme dans le cas d'une certitude absolue en la réalisation d'un
événement \footnote{Dans ce cas la certitude et la nécessité ont une
  valeur de 1.}, par conséquent :

\begin{equation}
  \forall A \in P(X), N(A) ≤ \Pi(A)
\end{equation}

De plus, comme la nécessité d'un événement est liée à la possibilité
de l'événement opposé (\autoref{eq:nec_comp}) et que la possiblité
d'un événement et de son opposé sont liées (\autoref{eq:poss_cont})
alors la nécessité et la possibilité sont également liées :

\begin{equation}
  \forall A \in P(X), \max(\Pi(A), 1-N(A)) = 1
\end{equation}

Cette caractéristique à deux conséquences majeures :

\begin{itemize}
\item S'il existe une incertitude alors la possibilité est maximale ;
  \(\text{si : } N(A) ≠ 0, \text{ alors : } Π(A)=1\)
\item Si l'événement n'est pas \enquote{tout à fait possible} alors il
  n'est pas nécessaire ;
  \(\text{si : } Π(A) ≠ 1, \text{ alors : } N(A)=0\)
\end{itemize}




La \emph{mesure de nécessité} est définissable à partir d'une
distribution de possibilité \(pi\).
%
Ainsi :
\begin{equation}
  N(A) = \inf_{x \notin A}(1-\pi(x))
\end{equation}




\subsubsection{Modélisation conjointe de \emph{l'incertitude} et de
\emph{l'imprécision}}


Dans le cas où \emph{l'incertitude} (\(i\)) est homogène, la
\emph{distribution de possibilité} (\(π\)) de cette proposition est
obtenue en tronquant la base de la fonction d'appartenance par la
droite d'ordonnée \(i\) \autocite{Bouchon-Meunier2007}. La
\emph{distribution de possibilité} (\(π'\)) combinant incerti XX est
donc :
%
\begin{equation}
  π'(x) = \max(π(x),\ i).  
\end{equation}


\begin{figure}
  \centering
  \input{../figures/fnc_incert_floue.tex}
  \caption{SS}
  \label{fig:ss}
\end{figure}


La confiance peut se définir en amont de la spatialisation.

Diminuer la confiance c'est augmenter le degré d'appartenance minimal

Proposer une version améliorée de la fig finale du chapitre 4 avec la
confiance.

Dire que les fonctions présentées reviennent a considérer que la
confiance est absolue

Parler des seuils de la confiance (3) et des valeurs associées ou
contraintes.


\subsection{La modélisation de la confiance d'un \emph{indice de
    localisation}}

Pour permettre la prise en compte de la confiance


La confiance donnée par le secouriste portant sur \emph{l'indice de
  localisation} et non sur l'une de ces composantes (\eg \emph{objet
  de référence,} \emph{relation de localisation,} \emph{etc.}), le
degré de confiance est distribué de manière homogène, chaque position
de la \ac{zir} se voyant attribuer la même valeur.


La modification des degrés d'appartenance des positions est
équivalente à une modification de la forme de la fonction
d'appartenance utilisée pour \emph{spatialiser l'indice de
  localisation} (\autoref{fig:fnc_incert}). Comme pour les
\emph{modifieurs} (\autoref{chap:07}), la variation du seuil de
confiance modifie la forme générale de la courbe, mais ne change pas
les paramètres du \emph{fuzzyficateur,} que sont l'écartement
(\(\delta\)) et la valeur de référence (\(v\)). Toutefois,
contrairement aux \emph{modifieurs,} la modification de la confiance à
également pour effet de changer la valeur de seuils de la fonction
d'appartenance. En effet, si la pente des fonctions appartenance est
conservée lors du rehaussement de la valeur minimale, ce n'est pas le
cas de la valeur où la fonction d'appartenance atteint la valeur
nulle, cette dernière étant rehaussée par la diminution de la
confiance.

\begin{figure}
  \centering  \subfloat[\label{fig:fnc_app_inc}]{\input{../figures/fnc_app_inc.tex}}\hfill
  \subfloat[\label{fig:fnc_app_inc_2}]{\input{../figures/fnc_app_inc_2.tex}}

  \subfloat[\label{fig:fnc_app_inc_3}]{\input{../figures/fnc_app_inc_3.tex}}\hfill
  \subfloat[\label{fig:fnc_app_inc_4}]{\input{../figures/fnc_app_inc_4.tex}}
  %
  \caption[Représentation des \emph{fonctions d'appartenance} de
  différents \emph{fuzzyfieurs} avec une
  \emph{incertitude}]{Représentation des \emph{fonctions
      d'appartenance} des \emph{fuzzyfieurs}
    \protect\onto[orla]{Eq\-Val} \protect\subref{fig:fnc_app_inc},
    \protect\onto[orla]{Sup\-Val} \protect\subref{fig:fnc_app_inc_2},
    \protect\onto[orla]{Inf\-Val} \protect\subref{fig:fnc_app_inc_3}
    et \protect\onto[orla]{Eq\-Val} avec le \emph{modifieur}
    \protect\onto[orla]{Not} \protect\subref{fig:fnc_app_inc_4}, avec
    un degré de \emph{certitude} (\(i\)) variable (respectivement
    0,8, 0,4, 0,2 et 0,5).}
  \label{fig:fnc_incert}
\end{figure}

On peut illustrer les effets de cette modélisation en comparant le
résultat de la \emph{spatialisation} d'un même \emph{indice de
  localisation} avec une confiance variable. La
\autoref{fig:zlc_cert_vs_inceret} illustre la \emph{spatialisation} de
la \emph{relation de localisation atomique} \onto[orla]{Pres\-De}
(comme pour les figures \ref{fig:tiers-exclu} et
\ref{fig:non-contradiction}) à partir du même pixel, avec la même
fonction d'appartenance. Cependant, alors qu'une confiance absolue est
donnée au premier cas, c'est une confiance partielle (0,4) qui est
donnée à la seconde modélisation. Par conséquent, la valeur minimale
que peut prendre une position est de 0,6. Si cette approche permet de
XXXXX, c'est par le jeu des opérations inter-zones qu'elle prend tout
son sens.

\begin{figure}
  \centering
  \subfloat[]{\input{../figures/zlc_certaine.tex}}\hspace{2cm}
  \subfloat[]{\input{../figures/zlc_incertaine.tex}}
  \caption{Modélisation d'un même \emph{relation de localisation
      atomique} (\protect\onto[orla]{Pres\-De}), à partir du même
    \emph{objet de référence} avec deux certitudes différentes,
    absolue et partielle (0,4).}
  \label{fig:zlc_cert_vs_inceret}
\end{figure}

En rehaussant la valeur du degré d'appartenance minimal à la \ac{zlc},
la m (\autoref{fig:intersection}).

\begin{figure}
  \centering
  \subfloat[]{\input{../figures/comparaison_intersection_incertitude.tex}}

  \subfloat[]{\input{../figures/comparaison_intersection_incertitude_2.tex}}
  \caption{Influence de l'incertitude d'un \emph{indice de
      localisation} sur la \emph{zone de localisation} résultant de la
    fusion de deux \ac{zlc}.}
  \label{fig:intersection}
\end{figure}



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../../../main"
%%% End:
