La méthode de construction des zones de localisation probables que
nous avons détaillé au cours des précédents chapitres ne remplit pas
encore tous les objectifs que nous avions fixés dans le
\autoref{chap:02}, nous ne pouvons pas encore prendre en compte la
confiance du secouriste en la qualité des indices qu'il traite.

Nous allons à présent détailler la manière dont nous modélisons cette
confiance, en commençant par présenter le cadre théorique sur lequel
nous nous appuyons, \emph{la théorie des possibilités} et la manière
dont il est utilisé pour ajouter la modélisation de la confiance à
notre méthode.

\subsection{La théorie des possibilités}

Le cadre théorique sur lequel nous avons bâti notre méthode, la
théorie des \emph{sous-ensembles flous} ne permet pas de modéliser
\emph{l'incertitude} d'une connaissance, seulement son
\emph{imprécision.} Il est ainsi impossible de traiter des situations
où les limites de la zone de localisation sont imprécises (\eg
\enquote{Je suis proche d'une ligne électrique}) et incertaines (\eg
\enquote{Je crois que je suis proche d'une ligne électrique}). Pour
permettre leur modélisation conjointe, \textcite{Zadeh1978} a proposé
la \emph{théorie des possibilités,} qui permet la modélisation de
\emph{l'incertitude} et qui, combinée à la théorie des sous-ensembles
flous nous permettra de modéliser des zones de localisation
imprécises, issues de la spatialisation d'indices de localisation
incertains.

Comme la \emph{théorie des probabilités,} la \emph{théorie des
  possibilités} permet de modéliser une incertitude sur la réalisation
d'un événement. Cependant ces deux théories utilisent un formalisme et
ont des propriétés différentes. Ces deux théories ne sont donc pas
équivalentes, même si elles s’inscrivent toutes deux dans le cadre
plus général \autocite{Bouchon-Meunier1995}, celui de la théorie des
fonctions de croyances \autocite{Shafer1976}. Le terme de
\emph{possibilité,} que nous allons régulièrement employer ci-dessous
ne doit donc pas être interprété comme un synonyme de probabilité,
mais bien comme un concept différent, que nous allons détailler.

\subsubsection{Mesures de possibilité et de nécessité}

La \emph{théorie des possibilités} permet d'évaluer un doute sur la
réalisation d'un (ou plusieurs) \emph{événement(s)}
donné(s). L'ensemble des événements étudiés sont regroupés dans un
ensemble fini \(X\), appelé \emph{ensemble des événements.} La
\emph{théorie des possibilités} permet d'attribuer à chaque événement
défini dans \(X\) un coefficient, compris entre 0 et 1, évaluant la
\emph{possibilité} de l'événement. Une valeur de 1 indiquant que
l'événement est \enquote{tout à fait possible}
\autocite[p. 43]{Bouchon-Meunier2007} et une valeur nulle, qu'il est
impossible. Bien que proches, la notion de possibilité ne doit pas
être confondue celle de probabilité. La probabilité quantifie, en
effet, la \enquote{chance} de réalisation d'un événement, alors que la
possibilité ne quantifie que la plausibilité de ce même événement. Dit
autrement, une probabilité maximale indique que l'événement considéré
va se réaliser, alors qu'une possibilité maximale n'indique que qu'il
certain que cet événement puisse se produire et non qu'il se produira.
Le coefficient de possibilité est attribué par une fonction \(Π\),
nommée \emph{mesure de possibilité,} qui l'attribue à chaque élément
de l'ensemble des parties de \emph{l'ensemble des événements}
(\(Π : P(X) → [0,1]\)). La fonction \(Π\) doit nécessairement :

\begin{itemize}
\item Attribuer un coefficient nul à \emph{l'ensemble vide} :
  \(Π(∅)=0\)
\item Attribuer un coefficient de 1 à \emph{l'ensemble des événements
    :} \(Π(X)=1\)
\item Définir la \emph{possibilité} conjonctive de deux événements
  (\(A\) et \(B\)) comme la \emph{possibilité} de l'événement le plus
  possible : \(∀ A,B ∈ P(X),\ Π(A ∪ B) = \max(Π(A),\ Π(B))\)
\end{itemize}

On peut déduire de ces trois propriétés \autocite{Bouchon-Meunier2007}
que la possibilité disjointe de deux événements \(A\) et \(B\) est
toujours inférieure à la \emph{possibilité} de l'événement le moins
possible :
%
\begin{equation}
  ∀ A,\ B ∈ P(X),\ Π(A ∩ B) ≤ \min(Π(A),\ Π(B))
\end{equation}
%
Il est par conséquent possible que deux événements aient
respectivement une possibilité non nulle, mais que leur occurrence
simultanée soit nulle (\( Π(A ∩ B) = 0\)) n'implique pas que \(Π(A)\)
et \(Π(B)\) soient égaux à 0). Une autre propriété déductible est que
les \emph{mesures de possibilités} sont \emph{monotones} relativement
à l'inclusion des parties de \(X\). Ce qui implique que si un
événement \(A\) est inclus dans un événement \(B\) (\eg si \(A\) est
la \emph{possibilité} qu'il pleuve en fin de journée, \(B\) pourrait
être la \emph{possibilité} que le temps change en fin de journée),
alors la possibilité de \(B\) est supérieure à celle de \(A\) :
%
\begin{equation}
  A \subseteq B, Π(A) ≤ Π(B)
\end{equation}
%
Ce qui revient à dire que la probabilité que le temps change (\(B\))
est supérieure à la \emph{possibilité} qu'il pleuve (\(A\)).

Une autre propriété des \emph{mesures de possibilités} est la faible
influence de la \emph{possibilité} d'un événement \(A\) sur la
\emph{possibilité} de son complémentaire \(A^C\). En effet, quelque
soit l’événement traité, lui ou son contraire est \enquote{tout à fait
  possible}, par conséquent :
%
\begin{equation}
  \label{eq:poss_cont}
  ∀ A ∈ P(X),\ \max(Π(A),\ Π(A^C)) = 1  
\end{equation}
%
Il est donc possible que la somme des \emph{possibilités} de \(A\) et
de \(A^C\) soit supérieure à 1. Cette propriété diffère fortement de
ce que l'on peut trouver en \emph{théories des probabilités} où la
probabilité du contraire d'un événement est plus contrainte, puisque
la somme de ces deux probabilités et toujours égale à 1. En
\emph{théorie des possibilités} il est simplement nécessaire qu'une
des \emph{possibilité} soit de 1. Il est par conséquent possible qu'un
événement et son contraire soient tous les deux parfaitement
possibles, ce qui traduit une situation d'ignorance absolue quant à la
réalisation de l'événement considéré.

\textcite{Bouchon-Meunier1995} illustre les \emph{mesures de
  possibilités} avec l'exemple de la réception d'un colis. On peut
définir une \emph{mesure de possibilité} donnant la susceptibilité que
le colis soit reçu un jour donné de la semaine. Dans ce cas,
\emph{l'ensemble des événements} \(X\) correspond à l'ensemble des
jours de la semaine et les parties de \(X\) aux combinaisons de ces
jours (\eg \(\{\text{lundi},\ \text{mardi}\}\),
\(\{\text{jeudi, vendredi, samedi}\}\), \emph{etc.})  On peut
attribuer une \emph{mesure de possibilité} à ces différentes parties
de \(X\). Si l'on reprend les valeurs proposées par
\textcite{Bouchon-Meunier1995} alors :
% 
\begin{itemize}
\item \(\Pi(\{\text{lundi, mardi}\})=1\)
\item \(\Pi(\{\text{mercredi}\})=0,8\)
\item \(\Pi(\{\text{jeudi}\})=0,2\)
\item \(\Pi(\{\text{vendredi, samedi, dimanche}\})=0\)
\end{itemize}
%
Ces valeurs signifient qu'il est \enquote{tout à fait possible} que le
colis arrive en début de semaine, relativement possible qu'il arrive
le mercredi, peu possible qu'il arrive le jeudi et impossible qu'il
arrive en fin de semaine. Comme on peut le remarquer il n'est pas
nécessaire d'attribuer une mesure de \emph{possibilité} à toutes les
parties de \(X\), cela n'empêche pas d'estimer les \emph{possibilités}
d'autres parties de \(X\), par exemple la \emph{possibilité} que le
colis arrive un mercredi ou un jeudi est de 0,8 \footnote{Soit le
  maximum des deux valeurs, conformément à la règle définissant la
  possibilité conjonctive.} ou la possibilité qu'il arrive le dimanche
est nulle \footnote{Étant donné que
  \(\Pi(\{\text{vendredi, samedi, dimanche}\})=0\) et que l'union
  possibilités est réalisée avec le maximum, cela implique que ni le
  vendredi, ni le samedi, ni le dimanche ont une possibilité non
  nulle.}. Il n'est cependant pas possible de connaitre précisément
certaines possibilités, comme par exemple celle que le colis arrive un
mardi. Il serait toutefois possible de calculer la possibilité de
chaque partie de \(X\) si l'on connaissait la possibilité de chaque
événement singleton (\ie la possibilité que le colis arrive lundi,
mardi, mercredi, \emph{etc.}), la possibilité des événements
composites (\eg la possibilité que le colis arrive lundi ou dimanche)
étant calculable en combinant les événements singletons.

Dans cette situation, c'est-à-dire lorsqu'une \emph{mesure de
  possibilité} attribue à chaque \emph{événement} singleton
\footnote{C'est-à-dire lorsque la possibilité n'est attribuée qu'aux
  éléments de \(X\) et non aux éléments des parties de \(X\).}) un
coefficient de possibilité, on parle de mesure de possibilité
\enquote{totalement définie} \autocite{Bouchon-Meunier2007}. Pour
attribuer un \emph{coefficient de possibilité} à chaque événement de
\(X\) on définit une \emph{distribution de possibilité}
(\(π : X → [0,1]\)), dont le supremum doit être égal à 1, c'est-à-dire
qu'au moins un événement (\(x\)) de l'ensemble considéré (\(X\)) doit
être totalement possible :

\begin{equation}
  \sup_{x ∈ X}π(x)=1
\end{equation}

À l'exception de cette dernière condition, toutes ces propriétés sont
partagées par les \emph{fonctions d'appartenance}
(\autoref{sec:3-2}). Ainsi, une \emph{fonction d'appartenance}
respectant cette dernière condition, c'est-à-dire qu'elle est
normalisée \footnote{Contrairement à une \emph{distribution de
    possibilité,} une fonction d'appartenance n'est pas tenue d'avoir
  1 pour supremum. On parle de \emph{sous-ensemble flou} normalisé
  quant au moins un de ces élément à un degré d’appartenance égal à un
  \autocite{Bouchon-Meunier2007}.}, est parfaitement équivalente à une
\emph{distribution de possibilités.} C'est par exemple le cas de
toutes les fonctions d'appartenance définies au cours du
\autoref{chap:07}, comme les fonctions \onto[orla]{Eq\-Val} ou
\onto[orla]{Sup\-Val}, mais aussi de leurs dériviés (\eg
\onto[orla]{Eq\-Val\-0}, \onto[orla]{Inf\-Val\-0}) ou des fonctions
d'appartenance obtenues par l’application d'un modifieur à une
fonction normalisée.

La \emph{mesure de possibilité} (\(\Pi\)) d'un événement donné (\(A\))
est reconstruction à partir d'une \emph{distribution de possibilités,}
elle est alors égale à la valeur la plus importante de la
\emph{distribution des possibilités} sur cet intervalle, soit :

\begin{equation}
  \Pi(A) = \sup_{x \in A}\pi(x)
\end{equation}

Pour approfondir notre précédent exemple, on peut définir une
\emph{distribution de possibilité} donnant la \emph{possibilité} qu'un
colis soit livré un jour donné (\autoref{fig:fnc_possib}). À partir
d'une telle fonction on peut calculer la \emph{mesure de possibilité}
de chaque partie de \emph{l'ensemble des événements.} Par exemple, la
possibilité que le \emph{colis} soit livré entre jeudi et dimanche et
de 0,2, puisque c'est la valeur maximale prise par la fonction \(\pi\)
sur cette période et la possibilité que le colis ne soit pas livré un
jeudi est égale à la valeur maximale de la distribution de possibilité
pour tous les jours à l'exception du jeudi, soit 1.

\begin{figure}
  \centering
  \input{../figures/fnc_possib.tex}
  \caption{Exemple d'une distribution de possibilité par paliers : la
    possibilité qu'un colis arrive chaque jours de la semaine.}
  \label{fig:fnc_possib}
\end{figure}

La mesure de possibilité ne permet pas à elle seule de quantifier la
\emph{certitude} d'un événement, elle ne renseigne que sur sa capacité
de réalisation. Si l'on reprend l’exemple du colis, mais en ne
conservant que la possibilité que le colis arrive le lundi. Le fait
que cette possibilité soit maximale ne donne pas suffisamment
d'informations pour savoir si le colis va réellement arriver le lundi,
tout dépend de la valeur prise par l'événement contraire (\ie le colis
n'arrive pas lundi). Si nous considérons que la possibilité que le
colis n'arrive pas lundi est nulle, alors cela traduit le fait que le
colis ne peut pas arriver un autre jour de la semaine, il peut donc
arriver le lundi (car : \(\Pi(\{\text{lundi}\}=1\)) et ne peut pas
arriver un autre jour (car : \(\Pi\{\text{mardi, …, dimanche}\}=0\)),
on est donc \emph{certains} que le colis arrivera lundi. À l'inverse,
si la possibilité que le colis n'arrive pas lundi est également de 1
(\ie \(\Pi\{\text{mardi, …, dimanche}\}=1\)) alors le colis peut aussi
bien arriver le lundi que n'importe quel autre jour de la semaine, on
n'a donc aucune idée de ce qui va réellement se produire, notre
incertitude sur cet événement est totale.

La certitude d'un événement ne peut donc être jugée qu'à l'aune de
deux mesures, la possibilité d'un événement et la possibilité de
l'événement contraire. On définit donc une seconde mesure, \emph{la
  nécessité,} quantifiant la certitude que l'on a sur la réalisation
d'un événement donné. La \emph{nécessité} est une mesure duale de la
possibilité et elle est exprimée à partir de la possibilité du
contraire de l'événement étudié :

\begin{equation}
  \label{eq:nec_comp}
  \forall A \in P(X), N(A) = 1 - \Pi(A^C)
\end{equation}

Comme la \emph{mesure de possibilité,} une \emph{mesure de nécessité}
est une fonction (\(N\)) attribuant à chaque partie de l'ensemble des
événements un degré, compris entre 0 et 1
(\(N : P(X) \rightarrow [0,1]\)). Cette fonction \(N\) doit également
:

\begin{itemize}
\item Attribuer un coefficient nul à \emph{l'ensemble vide} :
  \(N(∅)=0\)
\item Attribuer un coefficient de 1 à \emph{l'ensemble des événements
    :} \(N(X)=1\)
\item Définir la \emph{nécessité} disjonctive de deux événements
  (\(A\) et \(B\)) comme la \emph{nécessité} de l'événement le moins
  nécessaire : \(∀ A,B ∈ P(X),\ N(A \cap B) = \min(N(A),\ N(B))\)
\end{itemize}

Ce qui implique que plus l'événement complémentaire de \(A\) est
\emph{possible,} moins \(A\) est nécessaire. Pour reprendre l'exemple
précédent, si la \emph{possibilité} que le colis arrive lundi est de 1
et que la \emph{possibilité} qu'il n'arrive pas lundi est également de
1, alors la \emph{nécessité} de cet événement est nulle. Dans cette
configuration, il est parfaitement possible que le colis arrive lundi
(la possibilité est maximale) mais on n'en est absolument pas sûrs la
nécessité est nulle). Si la \emph{possibilité} que le colis n'arrive
pas lundi est nulle, alors la \emph{nécessité} qu'il arrive lundi est
de 1, c'est-à-dire qu'on a une confiance absolue dans la réalisation
de cet événement. La dualité de ces deux mesures contraint cependant
leurs valeurs respectives. Tout d'abord la \emph{nécessité} ne peut
pas être plus élevée que la \emph{possibilité} \footnote{Ce qui
  traduirait une situation, absurde, où un événement doit arriver mais
  ne le peut.}, mais les deux valeurs peuvent cependant être égales,
comme dans le cas d'une certitude absolue en la réalisation d'un
événement \footnote{Dans ce cas la certitude et la nécessité ont une
  valeur de 1.}, par conséquent :

\begin{equation}
  \forall A \in P(X), N(A) ≤ \Pi(A)
\end{equation}

De plus, comme la nécessité d'un événement est liée à la possibilité
de l'événement opposé (\autoref{eq:nec_comp}) et que la possiblité
d'un événement et de son opposé sont liées (\autoref{eq:poss_cont})
alors la nécessité et la possibilité sont également liées :

\begin{equation}
  \forall A \in P(X), \max(\Pi(A), 1-N(A)) = 1
\end{equation}

Cette caractéristique à deux conséquences majeures :

\begin{itemize}
\item S'il existe une incertitude alors la possibilité est maximale ;
  \(\text{si : } N(A) ≠ 0, \text{ alors : } Π(A)=1\)
\item Si l'événement n'est pas \enquote{tout à fait possible} alors il
  n'est pas nécessaire ;
  \(\text{si : } Π(A) ≠ 1, \text{ alors : } N(A)=0\)
\end{itemize}

Ces deux propriétés impliquent que dès qu'un événement est, ne
serai-ce qu'un peu, certain (\(N(A) ≠ 0\) alors il est parfaitement
possible (\(Π(A)=1\)). De même si un événement n'est pas parfaitement
possible (\(Π(A) ≠ 1\) alors on ne peut pas être certain
(\(N(A) = 0\)) qu'il se produira.

Étant donné que la nécessité sont des mesures duales, on peut définir
la mesure de nécessité (\(N(A)\)) à partir de la distribution de
possibilité (\(π\)) :

\begin{equation}
  N(A) = \inf_{x ∉ A}(1-π(x)) = 1 - \sup_{x ∉ A}π(x)
\end{equation}

Ainsi, une seule distribution (\(\pi\)) est nécessaire pour calculer
les mesures de possibilité et de nécessité de chaque événement. Par
exemple, si l'on reprend la distribution de possibilité proposée par
la \autoref{fig:fnc_possib}, on peut calculer la nécessité de chaque
événement. Par exemple, si la possibilité que le colis arrive le lundi
est de 1, la nécessité de ce même événement est de 0,2, puisque que le
minimum de la fonction \(1-π(x)\), pour tous les événements à
l'exception de celui étudié est de 0,2 (ce qui correspond a :
\(1-π(\{\text{mardi}\})\)).

\subsubsection{Modélisation conjointe de \emph{l'incertitude} et de
  \emph{l'imprécision}}

Pour permettre la conjointe de \emph{l'imprécision} et de
\emph{l'incertitude,} il est nécessaire de combiner le formalisme de
la théorie des possibilités à la théorie des sous-ensembles flous.

Comme nous l'avons déjà indiqué, une fonction d'appartenance
normalisée possède les mêmes caractéristiques qu'une distribution de
possibilité.
\tdi{Manque transition}
On peut alors définir la distribution de possibilité de la manière
suivante :

\begin{equation}
  \forall x \in X, \pi_{V, A}(x)=f_A(x)
\end{equation}

Ainsi, si un élément à un degré d'appartenance de 0,7 au sous-ensemble
flou \(A\), alors la possibilité que \(V\) prenne la valeur \(x\) est
également de 0,7 et sa nécessité est nulle.
%
Comme le signale \textcite{Bouchon-Meunier1995} la capacité de définir
une distribution de possibilité à partir d'une fonction d'appartenance
n'implique pas que ces deux fonctions doivent être identiques, il est
possible de combiner une distribution de possibilité et une fonction
d'appartenance différentes.
%
C'est par exemple le cas lorsque l'on cherche à définir une
incertitude s'appliquant de manière homogène à tous les éléments du
sous-ensemble flou considéré.

Dans le cas où une même valeur d'incertitude s'applique à tous les
éléments de l'ensemble de référence, alors aucun ce ses éléments ne
peut être qualifié d'impossible.
%
Admettons que l'on cherche à spatialiser l'indice de localisation :
\enquote{Elle est à 500 mètres du refuge}. Pour ce faire on recourra
la relation de localisation atomique
\onto[orla]{A\-Distance\-Planimetrique}, qui emploie le fuzzyficateur
\onto[orla]{Eq\-Val} avec une valeur de référence de 500 mètres. La
spatialisation de cette relation de localisation permet alors de
construire une \ac{zlc} représentée par un raster ne comprenant que
les pixels qui lui appartiennent, du moins partiellement
(\autoref{chap:06}). Dans le cas où ce même indice de localisation
n'est pas certain tout à fait certain (\eg \enquote{Je pense qu'elle
  est à 500 mètres du refuge}), il devient impossible d'affirmer qu'un
pixel donné n'appartient pas à la zone de localisation compatible. On
ne peut donc pas attribuer un degré d'appartenance nul aux pixels,
même s'ils sont situés à très grande distance de l'objet de
référence. Dans ce cas la \emph{distribution de possibilité} (\(π\))
de cette proposition est obtenue en tronquant la base de la fonction
d'appartenance par la droite d'ordonnée \(i\)
\autocite{Bouchon-Meunier2007}. La \emph{distribution de possibilité}
(\(π'\)) combinant une incertitude constante, de valeur \(i\) et une
imprécision modélisée par la distribution de possibilité \(\pi\) est
donc :

\begin{equation}
  \label{eq:incert_imp}
  π'(x) = \max(π(x),\ i).  
\end{equation}

On peut remarquer que dans cette situation où l'on modélise une
incertitude constante, la valeur de la \emph{mesure de nécessité,}
quantifiant la certitude, est égale au plus grand écart entre la
mesure de possibilité \footnote{Dont la valeur est nécessairement de 1
  puisqu’il y a incertitude.} et la fonction \(\pi'\). Comme la
nécessité mesure la certitude, plus cet écart est important, plus la
certitude est importante et le degré d'appartenance minimal est
faible, à l'inverse, plus cet écart est important, plus l'incertitude
est élevée, plus la valeur minimale prise par la fonction
d'appartenance \(\pi'\) est importante et les variations liées à la
distribution de possibilité \(\pi\) sont réduites.

La \autoref{fig:combinaison_incertitude_imprecision} illustre le
résultat d'une telle combinaison. On commence par définir une fonction
d'appartenance triangulaire et normalisée, équivalente à celle
proposée par le \emph{fuzzyficateur} \onto[orla]{Eq\-Val} et
permettant de modéliser une assertion logique telle que : \enquote{La
  \emph{métrique} à une valeur d'environ \(v\)}. On définit en
parallèle une distribution de possibilité homogène, de certitude
\(c\), indiquant qu'il est totalement possible et (faiblement dans cet
exemple) nécessaire que la \emph{métrique} prenne n'importe quelle
valeur de l'ensemble des valeurs possibles. On peut ensuite combiner
ces deux fonctions, pour obtenir la distribution de possibilité
\(\pi'\), correspondant à la fonction d’appartenance précédemment
définie, dont a diminué la \emph{certitude,} jusqu'à une valeur
\(c\). Comme le montre clairement la
\autoref{fig:combinaison_incertitude_imprecision}, il est ici
impossible d'obtenir un degré d'appartenance inférieur à l'incertitude
(\(1-c\)), quelle que soit la valeur de la \emph{métrique.} Les
\ac{zlc} construites à partir d'une fonction d'appartenance serons
donc constituées de tous les pixels de la \ac{zir}.

\begin{figure}
  \centering
  \input{../figures/fnc_incert_floue.tex}
  \caption{Illustration de la combinaison d'une distribution de
    possibilité constante et d'une fonction d'appartenance.}
  \label{fig:combinaison_incertitude_imprecision}
\end{figure}

\subsection{La modélisation de la confiance d'un \emph{indice de
    localisation}}

Dans notre cas, on cherche à utiliser la théorie des possibilités pour
représenter le doute du secouriste sur la véracité de l'indice de
localisation donné par le requérant. Ce parti-pris a donc une
implication majeure. Comme les sous-ensembles flous que nous
manipulons représentent des \emph{zones de localisation,} alors la
certitude s'applique de manière homogène à l'ensemble de
\emph{l'indice de localisation,} dit autrement nous nous plaçons dans
le cadre de l'équation \ref{eq:incert_imp}.

Pour prendre en compte la confiance du secouriste dans un indice de
localisation donné on peut employer la méthode décrite ci-dessus,
permettant d'aboutir à la
\autoref{fig:combinaison_incertitude_imprecision}. Comme pour les
modifieurs (\autoref{chap:07}), la modification de la certitude
implique une modification de la forme de la fonction d'appartenance
utilisée pour \emph{spatialiser l'indice de localisation,} mais cette
modification n'est pas réellement effectuée lors de la fusion, la
\ac{zlc} spatialisant \emph{l'indice de localisation} incertain est
simplement unie avec un raster homogène, attribuant ) chaque pixel un
degré d'appartenance égale à l'incertitude (\(1-c\)). Comme pour les
\emph{modifieurs,} on peut représenter le résultat de la modification
théoriques des fonctions d'appartenance. La \autoref{fig:fnc_incert}
représente la forme des principales fonctions d'appartenance définies
précédemment, avec une certitude variable. De manière similaire au
comportement des modificateurs, la diminution de la confiance modifie
la forme générale de la courbe, mais ne change pas les paramètres du
\emph{fuzzyficateur,} que sont l'écartement (\(\delta\)) et la valeur
de référence (\(v\)). Toutefois, contrairement aux \emph{modifieurs,}
la modification de la confiance à également pour effet de changer la
valeur de seuils de la fonction d'appartenance. En effet, si la pente
des fonctions appartenance est conservée lors du rehaussement de la
valeur minimale, ce n'est pas le cas de la valeur où la fonction
d'appartenance atteint la valeur nulle, cette dernière étant rehaussée
par la diminution de la confiance.

\begin{figure}
  \centering  \subfloat[\label{fig:fnc_app_inc}]{\input{../figures/fnc_app_inc.tex}}\hfill
  \subfloat[\label{fig:fnc_app_inc_2}]{\input{../figures/fnc_app_inc_2.tex}}

  \subfloat[\label{fig:fnc_app_inc_3}]{\input{../figures/fnc_app_inc_3.tex}}\hfill
  \subfloat[\label{fig:fnc_app_inc_4}]{\input{../figures/fnc_app_inc_4.tex}}
  %
  \caption[Représentation des \emph{fonctions d'appartenance} de
  différents \emph{fuzzyfieurs} avec une
  \emph{incertitude}]{Représentation des \emph{fonctions
      d'appartenance} des \emph{fuzzyfieurs}
    \protect\onto[orla]{Eq\-Val} \protect\subref{fig:fnc_app_inc},
    \protect\onto[orla]{Sup\-Val} \protect\subref{fig:fnc_app_inc_2},
    \protect\onto[orla]{Inf\-Val} \protect\subref{fig:fnc_app_inc_3}
    et \protect\onto[orla]{Eq\-Val} avec le \emph{modifieur}
    \protect\onto[orla]{Not} \protect\subref{fig:fnc_app_inc_4}, avec
    une \emph{certitude} (\(c\)) variable (respectivement 0,8, 0,4,
    0,2 et 0,5).}
  \label{fig:fnc_incert}
\end{figure}

On peut illustrer les effets de cette modélisation en comparant le
résultat de la \emph{spatialisation} d'un même \emph{indice de
  localisation} avec une confiance variable. La
\autoref{fig:zlc_cert_vs_inceret} illustre la \emph{spatialisation} de
la \emph{relation de localisation atomique} \onto[orla]{Pres\-De}
(comme pour les figures \ref{fig:tiers-exclu} et
\ref{fig:non-contradiction}) à partir du même pixel, avec la même
fonction d'appartenance. Cependant, alors qu'une confiance absolue est
donnée au premier cas, c'est une confiance partielle (0,4) qui est
donnée à la seconde modélisation. Comme nous l'indiquions
précédemment, on peut remarquer qu'aucune position ne possède un degré
d'appartenance inférieur à l'incertitude, ici 0,6, aucune position de
la \ac{zir} ne peut pas ne pas appartenir à la \ac{zlc}. On peut
remarquer que, la modification de la certitude à un impact conséquent
sur la \ac{zlc}. Réduire la confiance a donc un impact conséquent sur
les \ac{zlc} et par extension sur la \ac{zlp}, par le biais de
l'intersection des \ac{zlc}.

\begin{figure}
  \centering
  \subfloat[]{\input{../figures/zlc_certaine.tex}}\hspace{2cm}
  \subfloat[]{\input{../figures/zlc_incertaine.tex}}
  \caption{Modélisation d'un même \emph{relation de localisation
      atomique} (\protect\onto[orla]{Pres\-De}), à partir du même
    \emph{objet de référence} avec deux certitudes différentes,
    absolue et partielle (0,4).}
  \label{fig:zlc_cert_vs_inceret}
\end{figure}

Cette manière de traiter l'incertitude impose cependant de définir
manuellement une valeur de confiance, pour chaque indice, ce qui nous
semble nécessiter trop de manipulations pour une tâche devant être
réalisée dans des conditions pouvant être critiques. Nous proposons
donc de pré-définir des valeurs de certitude correspondant à des
descriptifs qualitatifs, tels que \enquote{confiance forte} ou
\enquote{confiance faible}, ce qui a pour effet de simplifier la
qualification des indices de localisation par les secouristes.

\subsubsection{L'influence de l'incertitude sur la fusion des zones de
  localisation compatibles}

La prise en compte de l'incertitude n'impacte pas seulement les
\emph{zones de localisation compatibles} représentant un indice de
localisation, mais également la \emph{zone de localisation probable}
construite par leur intersection.

La t-norme de \bsc{Zadeh} que nous utilisons pour les intersections
inter-\ac{zlc} a pour effet de retirer de la \ac{zlp} tout pixel ayant
un degré d'appartenance nul a une des \ac{zlc} intersectées.

La diminution de la certitude d'un indice de localisation a pour effet
d'attribuer un degré d'appartenance non nul à tous les pixels qui, en
cas de certitude absolue, auraient été considérés comme en dehors de
la \ac{zlc}. Ainsi, ces pixels, qui auraient été retirés de la
\ac{zlp} par le jeu des intersections peuvent à présent s'y retrouver,
à moins que leur degré d'appartenance à une seconde zone de
localisation compatible soit également nul.

La \autoref{fig:intersection_incert} représente le résultat de
l'intersection de deux zones de localisations compatibles, dont l'une
spatialise un indice de localisation de certitude variable. Dans le
premier cas (\ref{fig:intersection_incert_1}) la certitude dans les
deux \ac{zlc} intersectées est totale\footnote{Ce qui correspond à
  toutes les \ac{zlc} traitées jusqu'ici.}, certains pixels ont donc
un degré d'appartenance nul.
%
Dans le second cas (\ref{fig:intersection_incert_2}) la certitude de
la seconde \ac{zlc} est diminuée. Les pixels ayant un degré
d'appartenance inférieur à la valeur de l'incertitude (\ie
\(1-\text{certitude}\), ici 0,8) voient leur appartenance augmenter
jusqu'à cette valeur. Par conséquent, aucun des pixels représentant la
\ac{zir} n'a un degré d'appartenance nul. La seconde \ac{zlc} est
alors moins contraignante que dans le premier cas
(\ref{fig:intersection_incert_1}). Cela à deux effets, tout d'abord le
degré d'appartenance d'une position à la \ac{zlp} peut être amené à
augmenter si son degré d'appartenance était contraint par la seconde
\ac{zlc}. De plus, par corolaire, aucun pixel ne peut être retiré de
la \ac{zlp} par cette \ac{zlc}, aucun pixel n'ayant un degré
d'appartenance nul. Avec cette approche une \ac{zlc} spatialisant un
indice de localisation incertain ne pourra pas réduire la \ac{zlp},
seulement diminuer le degré d'appartenance de certaines positions.

\begin{figure}
  \centering  \subfloat[\label{fig:intersection_incert_1}]{\input{../figures/comparaison_intersection_incertitude.tex}}

  \subfloat[\label{fig:intersection_incert_2}]{\input{../figures/comparaison_intersection_incertitude_2.tex}}
  \caption{Résultat de l'intersection de deux \protect\ac{zlc} avec la
    \emph{t-norme} de \bsc{Zadeh} sans
    \protect\subref{fig:intersection_incert_1} et avec
    \protect\subref{fig:intersection_incert_2} incertitude.}
  \label{fig:intersection_incert}
\end{figure}

Dans le cas où tous les indices sont incertains, alors aucune position
de la \ac{zir} ne pourra avoir un degré d'appartenance nul. La
\ac{zlp} occupera alors tout l'espace de la \ac{zir}. Dans ces
conditions, seules les variations de degré d'appartenance permettent
d'identifier les régions correspondant le mieux à la description
donnée par le requérant.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../../../main"
%%% End:
